{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a285df3",
   "metadata": {},
   "source": [
    "## Case Study 2 : Final functions\n",
    "\n",
    "<br>\n",
    "<li> This notebook has two functions. function1 takes one or more inputs and predicts the output of % change and it's sign for those points. </li>\n",
    "<li> function2 takes three inputs X,Y1 and Y2. X is in in pandas dataframe format while Y1 & Y2 are in dict format specified in further part of this notebook and it returns mean deviation for Y1 while correct prediction ratio for Y2 with the pretrained model. </li>\n",
    "<li> Pretrained Multichannel convolution network with loss weights for balancing imbalance in sign prediction was used in this notebook. </li>\n",
    "<li> Model was saved in \"Handling_Imbalanced_data.ipynb\" while pickle files for scalars were saved at each instance they were used in \"EDA_and_Preprocessing_of_combined_Data.ipynb\". </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94f38192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "# for some models GPU capacity was not enough hence trained those models by disabling GPU with below line\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import re\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from tqdm import tqdm\n",
    "import tensorflow_text as text\n",
    "import pathlib\n",
    "import tensorflow_addons as tfa\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b3efee",
   "metadata": {},
   "source": [
    "### 1. Defining function1 and function2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7594d359",
   "metadata": {},
   "source": [
    "### 1.1 \"function1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "244d5294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function1(X):\n",
    "    '''\n",
    "        1.  This function takes input in pandas dataframe format with following columns and predicts\n",
    "            the % chnage in closing price and sign of change.\n",
    "            \n",
    "            ['ticker_symbol', 'post_date', 'body', 'comment_num', 'retweet_num','like_num']\n",
    "        \n",
    "        2.  The input can be of one date,company or multiple.\n",
    "    '''\n",
    "    # Extracting features from input\n",
    "    \n",
    "    # function to detect whether URL is present or not.\n",
    "    def Find_url(string):  \n",
    "        return ('https' in string or 'http' in string)\n",
    "\n",
    "    X['URL_flag'] = X.body.apply(lambda x:1 if Find_url(x) else 0)\n",
    "    \n",
    "    # fuunction to know whether hashtags are present in tweet text or not\n",
    "    def Find_hashtag(string):\n",
    "        temp = re.search(r\"#(\\w+)\", string)     \n",
    "        return temp\n",
    "\n",
    "    # Extracting hashtag_flag feature\n",
    "    X['hastags_flag'] = X.body.apply(lambda x:1 if Find_hashtag(x) else 0)\n",
    "    \n",
    "    # referred and modified below link to extract hashtags from tweets\n",
    "    # https://www.geeksforgeeks.org/python-extract-hashtags-from-text/\n",
    "    def Find_mention(string):\n",
    "        temp = re.search(r\"@(\\w+)\", string)     \n",
    "        return temp\n",
    "\n",
    "    # extracting mention_flag features\n",
    "    X['mention_flag'] = X.body.apply(lambda x:1 if Find_mention(x) else 0)\n",
    "    \n",
    "    # referred below link to extract hashtags from tweets\n",
    "    # https://www.geeksforgeeks.org/python-extract-hashtags-from-text/\n",
    "\n",
    "    def get_hashtag(string):\n",
    "        hashtags  = re.findall(r\"#(\\w+)\", string)\n",
    "        return hashtags\n",
    "\n",
    "    # extracting and storing hashtags in dataframe\n",
    "    X['hashtags'] = X.body.apply(lambda x:','.join(get_hashtag(x)))\n",
    "    \n",
    "    # referred stopwords from below link\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "\n",
    "\n",
    "    # referred this cleaning function from Donor Choose assignments\n",
    "    def preprocess(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r\"#(\\w+)\", '', text)\n",
    "        text = re.sub(r\"@(\\w+)\", '', text)\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "        text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'s\", \" is\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "        text = re.sub(r\"\\'t\", \" not\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"\\'m\", \" am\", text)\n",
    "        text = text.replace('\\\\r', ' ')\n",
    "        text = text.replace('\\\\n', ' ')\n",
    "        text = text.replace('\\\\\"', ' ')\n",
    "        text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "        text = text.replace('\\\\r', ' ')\n",
    "        text = text.replace('\\\\n', ' ')\n",
    "        text = text.replace('\\\\\"', ' ')\n",
    "        text = ' '.join(e for e in text.split() if e.lower() not in stopwords)\n",
    "        return text\n",
    "\n",
    "\n",
    "    # cleaning tweet text and storing in 'tweet_cleaned' column\n",
    "    X['tweet_cleaned'] = X.body.apply(lambda x:preprocess(x))\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    senti_score_train = [sid.polarity_scores(x_body) for x_body in X['tweet_cleaned']]\n",
    "\n",
    "\n",
    "    X['neg'] = [senti['neg'] for senti in senti_score_train]\n",
    "    X['neu'] = [senti['neu'] for senti in senti_score_train]\n",
    "    X['pos'] = [senti['pos'] for senti in senti_score_train]\n",
    "    X['compound'] = [senti['compound'] for senti in senti_score_train]\n",
    "    \n",
    "    [retweet_num_scalar,comment_num_scalar,like_num_scalar] = pickle.load(open(\"scalars.pkl\",\"rb\"))\n",
    "\n",
    "    X['retweet_num'] = retweet_num_scalar.fit_transform(X['retweet_num'].values.reshape(-1,1))\n",
    "    X['comment_num'] = comment_num_scalar.fit_transform(X['comment_num'].values.reshape(-1,1))\n",
    "    X['like_num'] = like_num_scalar.fit_transform(X['like_num'].values.reshape(-1,1))\n",
    "    \n",
    "    X['hashtags'] = X.hashtags.apply(lambda x:0 if len(x)<1 else x)\n",
    "    \n",
    "    keep_indices = []\n",
    "    for i in range(X.shape[0]):\n",
    "        if len(X.iloc[0]['tweet_cleaned'])>1:\n",
    "            keep_indices.append(i)\n",
    "    keep_indices = np.array(keep_indices)\n",
    "    X = X.iloc[keep_indices]\n",
    "    \n",
    "\n",
    "    dates = X['post_date'].unique()\n",
    "    companies = X['ticker_symbol'].unique()  \n",
    "\n",
    "    # empty list to store combined tweet text\n",
    "    tweets = {}\n",
    "\n",
    "    for date in dates:\n",
    "        tweet_data = X[X.post_date == date]\n",
    "        # empty string to store and accumulate tweet text for a day\n",
    "        all_tweets = ''\n",
    "        # loop to iterate through all tweets on that day\n",
    "        for tweet in tweet_data['tweet_cleaned']: \n",
    "            all_tweets += tweet\n",
    "\n",
    "        # storing all tweets to tweets dictionary\n",
    "        tweets[date] = all_tweets\n",
    "\n",
    "    \n",
    "    # referred this preprocess function from Donor Choose assignments\n",
    "    def preprocess(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "        text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'s\", \" is\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "        text = re.sub(r\"\\'t\", \" not\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"\\'m\", \" am\", text)\n",
    "        text = text.replace('\\\\r', ' ')\n",
    "        text = text.replace('\\\\n', ' ')\n",
    "        text = text.replace('\\\\\"', ' ')\n",
    "        text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "        return text\n",
    "\n",
    "    # Getting Glove vec dictionary\n",
    "    with open('glove_vectors', 'rb') as f:\n",
    "        glove_dict = pickle.load(f)\n",
    "\n",
    "    # Function to get hashtag vectors\n",
    "    def vec_hashtag(hashtags):\n",
    "        vec = np.zeros(300)   # empty vector of dimension 300\n",
    "        n_letters = 1         # counting letters in all hashtags\n",
    "        if hashtags:\n",
    "            hashtags = hashtags.split(',')         # getting all hashtags for current tweet\n",
    "            for hashtag in hashtags:               # loop to iterate through all hashtags\n",
    "                hashtag = preprocess(hashtag)      # preprocessing all hashtags\n",
    "                hashtag = hashtag.replace(\" \", \"\") # removing spaces in hashtags\n",
    "                for letter in hashtag:           \n",
    "                    vec += glove_dict[letter]\n",
    "                    n_letters += 1\n",
    "            vec /= n_letters                       # finding average of all letter vectors\n",
    "        return np.array(vec)\n",
    "\n",
    "    # referred array padding from below two links\n",
    "    # https://numpy.org/doc/stable/reference/generated/numpy.pad.html\n",
    "    # https://stackoverflow.com/questions/35751306/python-how-to-pad-numpy-array-with-zeros\n",
    "\n",
    "    feature_mat = {}         # emtpy list to store all feature matrices\n",
    "    req_dim = 500            # selected tweets for a day\n",
    "    for date in dates: # loop to iterate through all dates\n",
    "        vec_data = X[X.post_date == date]    # filtering data for current date\n",
    "\n",
    "        # getting feature vector for current date with all other features\n",
    "        temp = vec_data.drop(['body','ticker_symbol','hashtags','post_date','tweet_cleaned'],axis=1).values\n",
    "\n",
    "        # empty list to store hashtag vector\n",
    "        hash_vec = []\n",
    "\n",
    "        # loop to iterate thorugh all tweet hashtags and getting their vector representation\n",
    "        for tweet in vec_data['hashtags'].values:\n",
    "            hash_vec.append(vec_hashtag(tweet))\n",
    "        hash_vec = np.array(hash_vec)\n",
    "\n",
    "        # combining hashtag vectors and other feature vectors for current date\n",
    "        hash_vec = np.hstack((temp,hash_vec))\n",
    "\n",
    "        try:       # padding matrix with 0's if number of tweet vectors are less than req_dim\n",
    "            hash_vec = np.pad(hash_vec, [(0, req_dim-hash_vec.shape[0]), (0, 0)])\n",
    "        except:    # else select first 500 tweet vectors\n",
    "            hash_vec = hash_vec[:req_dim,:]\n",
    "\n",
    "        # adding feature matrix to defined list\n",
    "        # added one more dimension so that while training neural network, these arrrays will be treated as single channel images\n",
    "        feature_mat[date] = np.expand_dims(hash_vec,axis=-1)\n",
    "    \n",
    "    structured_data = pd.DataFrame(columns=['post_date','tweet_text','company_name','feat_mat'])\n",
    "    \n",
    "    for company in companies:\n",
    "        for date in dates:\n",
    "            structured_data = structured_data.append({'post_date':date,\n",
    "                                    'tweet_text':tweets[date],\n",
    "                                    'company_name':company,\n",
    "                                    'feat_mat':feature_mat[date]},ignore_index=True)\n",
    "    \n",
    "    x_text = structured_data['tweet_text'].values\n",
    "    x_feat = list(structured_data['feat_mat'].values)\n",
    "    x_feat = np.array(x_feat)\n",
    "    x_company = structured_data['company_name'].values\n",
    "    \n",
    "    bert_tokenizer_params=dict(lower_case=True)\n",
    "    reserved_tokens=[]\n",
    "\n",
    "    bert_vocab_args = dict(\n",
    "        # The target vocabulary size\n",
    "        vocab_size = 8000,\n",
    "        # Reserved tokens that must be included in the vocabulary\n",
    "        reserved_tokens=reserved_tokens,\n",
    "        # Arguments for `text.BertTokenizer`\n",
    "        bert_tokenizer_params=bert_tokenizer_params,\n",
    "        # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "        learn_params={},\n",
    "    )\n",
    "\n",
    "    # creating BertTokenizer object with vocab text file genrated as per reference link stated above\n",
    "    en_tokenizer = text.BertTokenizer('en_vocab.txt', **bert_tokenizer_params)\n",
    "    vocab_size_text = len(pathlib.Path('en_vocab.txt').read_text().splitlines())+ 1\n",
    "    \n",
    "    # Using BertTokenizer to tokenize train data\n",
    "    encoded_text = en_tokenizer.tokenize(x_text)\n",
    "    encoded_text = encoded_text.merge_dims(-2,-1)   # reducing dimension of ragged tensor\n",
    "    encoded_text = encoded_text.to_list()           # converting to list to pad the sequences\n",
    "    max_length = 5000                               # max length of padding\n",
    "    x_text = pad_sequences(encoded_text, maxlen=max_length, padding='post')\n",
    "    \n",
    "    tokenizer = pickle.load(open(\"tokenizer.pkl\",\"rb\"))\n",
    "    # getting tokenized train data\n",
    "    train_comp = np.array(tokenizer.texts_to_sequences(x_company))\n",
    "    \n",
    "    loaded_model = load_model(\"trained_model.h5\")\n",
    "    prediction = (x_company,loaded_model.predict([x_text,x_feat,train_comp]))\n",
    "    \n",
    "    prediction[1][0] = [i[0] for i in prediction[1][0]]\n",
    "    prediction[1][1] = [1 if i[0]>=0.5 else 0 for i in prediction[1][1]]\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4f3be1",
   "metadata": {},
   "source": [
    "### 1.2 \"function2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "128183e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function2(X,Y1,Y2):\n",
    "    '''\n",
    "        1.  This function takes input in pandas dataframe format with following columns and gives outputs\n",
    "            as mean deviation in chnage prediction and ratio of correct signs predicted.\n",
    "            \n",
    "            ['ticker_symbol', 'post_date', 'body', 'comment_num', 'retweet_num','like_num']\n",
    "        \n",
    "        2.  The input can be of multiple dates and companies.\n",
    "    '''\n",
    "    \n",
    "    def Find_url(string):  \n",
    "        return ('https' in string or 'http' in string)\n",
    "\n",
    "    X['URL_flag'] = X.body.apply(lambda x:1 if Find_url(x) else 0)\n",
    "    \n",
    "    # fuunction to know whether hashtags are present in tweet text or not\n",
    "    def Find_hashtag(string):\n",
    "        temp = re.search(r\"#(\\w+)\", string)     \n",
    "        return temp\n",
    "\n",
    "    # Extracting hashtag_flag feature\n",
    "    X['hastags_flag'] = X.body.apply(lambda x:1 if Find_hashtag(x) else 0)\n",
    "    \n",
    "    # referred and modified below link to extract hashtags from tweets\n",
    "    # https://www.geeksforgeeks.org/python-extract-hashtags-from-text/\n",
    "    def Find_mention(string):\n",
    "        temp = re.search(r\"@(\\w+)\", string)     \n",
    "        return temp\n",
    "\n",
    "    # extracting mention_flag features\n",
    "    X['mention_flag'] = X.body.apply(lambda x:1 if Find_mention(x) else 0)\n",
    "    \n",
    "    # referred below link to extract hashtags from tweets\n",
    "    # https://www.geeksforgeeks.org/python-extract-hashtags-from-text/\n",
    "\n",
    "    def get_hashtag(string):\n",
    "        hashtags  = re.findall(r\"#(\\w+)\", string)\n",
    "        return hashtags\n",
    "\n",
    "    # extracting and storing hashtags in dataframe\n",
    "    X['hashtags'] = X.body.apply(lambda x:','.join(get_hashtag(x)))\n",
    "    \n",
    "    stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "\n",
    "\n",
    "    # referred this cleaning function from Donor Choose assignments\n",
    "    def preprocess(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r\"#(\\w+)\", '', text)\n",
    "        text = re.sub(r\"@(\\w+)\", '', text)\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "        text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'s\", \" is\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "        text = re.sub(r\"\\'t\", \" not\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"\\'m\", \" am\", text)\n",
    "        text = text.replace('\\\\r', ' ')\n",
    "        text = text.replace('\\\\n', ' ')\n",
    "        text = text.replace('\\\\\"', ' ')\n",
    "        text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "        text = text.replace('\\\\r', ' ')\n",
    "        text = text.replace('\\\\n', ' ')\n",
    "        text = text.replace('\\\\\"', ' ')\n",
    "            # https://gist.github.com/sebleier/554280\n",
    "        text = ' '.join(e for e in text.split() if e.lower() not in stopwords)\n",
    "        return text\n",
    "\n",
    "\n",
    "    # cleaning tweet text and storing in 'tweet_cleaned' column\n",
    "    X['tweet_cleaned'] = X.body.apply(lambda x:preprocess(x))\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    senti_score_train = [sid.polarity_scores(x_body) for x_body in tqdm(X['tweet_cleaned'])]\n",
    "\n",
    "\n",
    "    X['neg'] = [senti['neg'] for senti in senti_score_train]\n",
    "    X['neu'] = [senti['neu'] for senti in senti_score_train]\n",
    "    X['pos'] = [senti['pos'] for senti in senti_score_train]\n",
    "    X['compound'] = [senti['compound'] for senti in senti_score_train]\n",
    "    \n",
    "    X = X[X.retweet_num > 1].copy()\n",
    "    [retweet_num_scalar,comment_num_scalar,like_num_scalar] = pickle.load(open(\"scalars.pkl\",\"rb\"))\n",
    "\n",
    "    X['retweet_num'] = retweet_num_scalar.transform(X['retweet_num'].values.reshape(-1,1))\n",
    "    X['comment_num'] = comment_num_scalar.transform(X['comment_num'].values.reshape(-1,1))\n",
    "    X['like_num'] = like_num_scalar.transform(X['like_num'].values.reshape(-1,1))\n",
    "    \n",
    "    X['hashtags'] = X.hashtags.apply(lambda x:0 if len(x)<1 else x)\n",
    "    \n",
    "    keep_indices = []\n",
    "    for i in range(X.shape[0]):\n",
    "        if len(X.iloc[0]['tweet_cleaned'])>1:\n",
    "            keep_indices.append(i)\n",
    "    keep_indices = np.array(keep_indices)\n",
    "    X = X.iloc[keep_indices]\n",
    "    \n",
    "\n",
    "    dates = X['post_date'].unique()\n",
    "    companies = X['ticker_symbol'].unique()  \n",
    "\n",
    "    # empty list to store combined tweet text\n",
    "    tweets = {}\n",
    "\n",
    "    for date in tqdm(dates):\n",
    "        tweet_data = X[X.post_date == date]\n",
    "        # empty string to store and accumulate tweet text for a day\n",
    "        all_tweets = ''\n",
    "        # loop to iterate through all tweets on that day\n",
    "        for tweet in tweet_data['tweet_cleaned']: \n",
    "            all_tweets += tweet\n",
    "\n",
    "        # storing all tweets to tweets dictionary\n",
    "        tweets[date] = all_tweets\n",
    "\n",
    "    \n",
    "    # referred this preprocess function from Donor Choose assignments\n",
    "    def preprocess(text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "        text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'s\", \" is\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "        text = re.sub(r\"\\'t\", \" not\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"\\'m\", \" am\", text)\n",
    "        text = text.replace('\\\\r', ' ')\n",
    "        text = text.replace('\\\\n', ' ')\n",
    "        text = text.replace('\\\\\"', ' ')\n",
    "        text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "        return text\n",
    "\n",
    "    # Getting Glove vec dictionary\n",
    "    with open('glove_vectors', 'rb') as f:\n",
    "        glove_dict = pickle.load(f)\n",
    "\n",
    "    # Function to get hashtag vectors\n",
    "    def vec_hashtag(hashtags):\n",
    "        vec = np.zeros(300)   # empty vector of dimension 300\n",
    "        n_letters = 1         # counting letters in all hashtags\n",
    "        if hashtags:\n",
    "            hashtags = hashtags.split(',')         # getting all hashtags for current tweet\n",
    "            for hashtag in hashtags:               # loop to iterate through all hashtags\n",
    "                hashtag = preprocess(hashtag)      # preprocessing all hashtags\n",
    "                hashtag = hashtag.replace(\" \", \"\") # removing spaces in hashtags\n",
    "                for letter in hashtag:           \n",
    "                    vec += glove_dict[letter]\n",
    "                    n_letters += 1\n",
    "            vec /= n_letters                       # finding average of all letter vectors\n",
    "        return np.array(vec)\n",
    "\n",
    "    # referred array padding from below two links\n",
    "    # https://numpy.org/doc/stable/reference/generated/numpy.pad.html\n",
    "    # https://stackoverflow.com/questions/35751306/python-how-to-pad-numpy-array-with-zeros\n",
    "\n",
    "    feature_mat = {}         # emtpy list to store all feature matrices\n",
    "    req_dim = 500            # selected tweets for a day\n",
    "    for date in tqdm(dates): # loop to iterate through all dates\n",
    "        vec_data = X[X.post_date == date]    # filtering data for current date\n",
    "\n",
    "        # getting feature vector for current date with all other features\n",
    "        temp = vec_data.drop(['body','ticker_symbol','hashtags','post_date','tweet_cleaned'],axis=1).values\n",
    "\n",
    "        # empty list to store hashtag vector\n",
    "        hash_vec = []\n",
    "\n",
    "        # loop to iterate thorugh all tweet hashtags and getting their vector representation\n",
    "        for tweet in vec_data['hashtags'].values:\n",
    "            hash_vec.append(vec_hashtag(tweet))\n",
    "        hash_vec = np.array(hash_vec)\n",
    "\n",
    "        # combining hashtag vectors and other feature vectors for current date\n",
    "        hash_vec = np.hstack((temp,hash_vec))\n",
    "\n",
    "        try:       # padding matrix with 0's if number of tweet vectors are less than req_dim\n",
    "            hash_vec = np.pad(hash_vec, [(0, req_dim-hash_vec.shape[0]), (0, 0)])\n",
    "        except:    # else select first 500 tweet vectors\n",
    "            hash_vec = hash_vec[:req_dim,:]\n",
    "\n",
    "        # adding feature matrix to defined list\n",
    "        # added one more dimension so that while training neural network, these arrrays will be treated as single channel images\n",
    "        feature_mat[date] = np.expand_dims(hash_vec,axis=-1)\n",
    "    \n",
    "    structured_data = pd.DataFrame(columns=['post_date','tweet_text','company_name','feat_mat'])\n",
    "    \n",
    "    for company in companies:\n",
    "        for date in tqdm(dates):\n",
    "            structured_data = structured_data.append({'post_date':date,\n",
    "                                    'tweet_text':tweets[date],\n",
    "                                    'company_name':company,\n",
    "                                    'feat_mat':feature_mat[date]},ignore_index=True)\n",
    "    x_dates = structured_data['post_date'].values\n",
    "    x_text = structured_data['tweet_text'].values\n",
    "    x_feat = list(structured_data['feat_mat'].values)\n",
    "    x_feat = np.array(x_feat)\n",
    "    x_company = structured_data['company_name'].values\n",
    "    \n",
    "    \n",
    "    bert_tokenizer_params=dict(lower_case=True)\n",
    "    reserved_tokens=[]\n",
    "\n",
    "    bert_vocab_args = dict(\n",
    "        # The target vocabulary size\n",
    "        vocab_size = 8000,\n",
    "        # Reserved tokens that must be included in the vocabulary\n",
    "        reserved_tokens=reserved_tokens,\n",
    "        # Arguments for `text.BertTokenizer`\n",
    "        bert_tokenizer_params=bert_tokenizer_params,\n",
    "        # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "        learn_params={},\n",
    "    )\n",
    "\n",
    "    # creating BertTokenizer object with vocab text file genrated as per reference link stated above\n",
    "    en_tokenizer = text.BertTokenizer('en_vocab.txt', **bert_tokenizer_params)\n",
    "    vocab_size_text = len(pathlib.Path('en_vocab.txt').read_text().splitlines())+ 1\n",
    "    \n",
    "    # Using BertTokenizer to tokenize train data\n",
    "    encoded_text = en_tokenizer.tokenize(x_text)\n",
    "    encoded_text = encoded_text.merge_dims(-2,-1)   # reducing dimension of ragged tensor\n",
    "    encoded_text = encoded_text.to_list()           # converting to list to pad the sequences\n",
    "    max_length = 5000                               # max length of padding\n",
    "    x_text = pad_sequences(encoded_text, maxlen=max_length, padding='post')\n",
    "    \n",
    "    tokenizer = pickle.load(open(\"tokenizer.pkl\",\"rb\"))\n",
    "    # getting tokenized train data\n",
    "    train_comp = np.array(tokenizer.texts_to_sequences(x_company))\n",
    "    \n",
    "    loaded_model = load_model(\"trained_model.h5\")\n",
    "    prediction = (x_company,loaded_model.predict([x_text,x_feat,train_comp]))\n",
    "    \n",
    "    deviation = []\n",
    "\n",
    "    count = 0\n",
    "    sign_pred = [1 if i[0]>=0.5 else 0 for i in prediction[1][1]]\n",
    "    for i,date,company in zip(range(len(x_dates)),x_dates,x_company):\n",
    "        if Y1[date][company] == -1 or Y2[date][company] == -1:\n",
    "            continue\n",
    "        deviation.append(abs(prediction[1][0][i]-Y1[date][company]))\n",
    "        if sign_pred[i] == Y2[date][company]:\n",
    "            count += 1   \n",
    "            \n",
    "    print(\"Mean Deviation for % change in closing prices is : \",np.mean(deviation))\n",
    "    print(\"Number of correct signs predicted out of {} are : {}\".format(len(sign_pred),count))\n",
    "    return [np.mean(deviation),count/len(sign_pred)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e79661",
   "metadata": {},
   "source": [
    "## 2. Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f74fbfd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>ticker_symbol</th>\n",
       "      <th>writer</th>\n",
       "      <th>post_date</th>\n",
       "      <th>body</th>\n",
       "      <th>comment_num</th>\n",
       "      <th>retweet_num</th>\n",
       "      <th>like_num</th>\n",
       "      <th>close_value</th>\n",
       "      <th>volume</th>\n",
       "      <th>open_value</th>\n",
       "      <th>high_value</th>\n",
       "      <th>low_value</th>\n",
       "      <th>close_value_change</th>\n",
       "      <th>change_label</th>\n",
       "      <th>class_sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>550803612197457920</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>SentiQuant</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>#TOPTICKERTWEETS $AAPL $IMRS $BABA $EBAY $AMZN...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110.38</td>\n",
       "      <td>41304780</td>\n",
       "      <td>112.82</td>\n",
       "      <td>113.13</td>\n",
       "      <td>110.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>550803610825928706</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>SentiQuant</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>#SENTISHIFTUP $K $FB $GOOGL $GS $GOLD $T $AAPL...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110.38</td>\n",
       "      <td>41304780</td>\n",
       "      <td>112.82</td>\n",
       "      <td>113.13</td>\n",
       "      <td>110.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>550803225113157632</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>MacHashNews</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Rumor Roundup: What to expect when you're expe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110.38</td>\n",
       "      <td>41304780</td>\n",
       "      <td>112.82</td>\n",
       "      <td>113.13</td>\n",
       "      <td>110.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>550802957370159104</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>WaltLightShed</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>An $AAPL store line in Sapporo Japan for the \"...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>110.38</td>\n",
       "      <td>41304780</td>\n",
       "      <td>112.82</td>\n",
       "      <td>113.13</td>\n",
       "      <td>110.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>550802855129382912</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2waystrading</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>$AAPL - Will $AAPL Give Second entry opportuni...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110.38</td>\n",
       "      <td>41304780</td>\n",
       "      <td>112.82</td>\n",
       "      <td>113.13</td>\n",
       "      <td>110.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id ticker_symbol         writer   post_date  \\\n",
       "0  550803612197457920          AAPL     SentiQuant  2015-01-01   \n",
       "1  550803610825928706          AAPL     SentiQuant  2015-01-01   \n",
       "2  550803225113157632          AAPL    MacHashNews  2015-01-01   \n",
       "3  550802957370159104          AAPL  WaltLightShed  2015-01-01   \n",
       "4  550802855129382912          AAPL   2waystrading  2015-01-01   \n",
       "\n",
       "                                                body  comment_num  \\\n",
       "0  #TOPTICKERTWEETS $AAPL $IMRS $BABA $EBAY $AMZN...            0   \n",
       "1  #SENTISHIFTUP $K $FB $GOOGL $GS $GOLD $T $AAPL...            0   \n",
       "2  Rumor Roundup: What to expect when you're expe...            0   \n",
       "3  An $AAPL store line in Sapporo Japan for the \"...            2   \n",
       "4  $AAPL - Will $AAPL Give Second entry opportuni...            0   \n",
       "\n",
       "   retweet_num  like_num  close_value    volume  open_value  high_value  \\\n",
       "0            0         1       110.38  41304780      112.82      113.13   \n",
       "1            0         1       110.38  41304780      112.82      113.13   \n",
       "2            0         0       110.38  41304780      112.82      113.13   \n",
       "3            4         4       110.38  41304780      112.82      113.13   \n",
       "4            0         0       110.38  41304780      112.82      113.13   \n",
       "\n",
       "   low_value  close_value_change  change_label  class_sign  \n",
       "0     110.21                 0.0           0.0           1  \n",
       "1     110.21                 0.0           0.0           1  \n",
       "2     110.21                 0.0           0.0           1  \n",
       "3     110.21                 0.0           0.0           1  \n",
       "4     110.21                 0.0           0.0           1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the combined tweet data from all files \n",
    "\n",
    "data = pd.read_csv('final_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b8a7b3",
   "metadata": {},
   "source": [
    "<li> We can see there are many columns in this combined data and are not required for model to predict. </li>\n",
    "<li> What model requires is to predict the output based on tweets on a particular day. </li>\n",
    "<li> Hence let us drop the non required columns. </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99aba788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker_symbol</th>\n",
       "      <th>post_date</th>\n",
       "      <th>body</th>\n",
       "      <th>comment_num</th>\n",
       "      <th>retweet_num</th>\n",
       "      <th>like_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>#TOPTICKERTWEETS $AAPL $IMRS $BABA $EBAY $AMZN...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>#SENTISHIFTUP $K $FB $GOOGL $GS $GOLD $T $AAPL...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>Rumor Roundup: What to expect when you're expe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>An $AAPL store line in Sapporo Japan for the \"...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>$AAPL - Will $AAPL Give Second entry opportuni...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker_symbol   post_date  \\\n",
       "0          AAPL  2015-01-01   \n",
       "1          AAPL  2015-01-01   \n",
       "2          AAPL  2015-01-01   \n",
       "3          AAPL  2015-01-01   \n",
       "4          AAPL  2015-01-01   \n",
       "\n",
       "                                                body  comment_num  \\\n",
       "0  #TOPTICKERTWEETS $AAPL $IMRS $BABA $EBAY $AMZN...            0   \n",
       "1  #SENTISHIFTUP $K $FB $GOOGL $GS $GOLD $T $AAPL...            0   \n",
       "2  Rumor Roundup: What to expect when you're expe...            0   \n",
       "3  An $AAPL store line in Sapporo Japan for the \"...            2   \n",
       "4  $AAPL - Will $AAPL Give Second entry opportuni...            0   \n",
       "\n",
       "   retweet_num  like_num  \n",
       "0            0         1  \n",
       "1            0         1  \n",
       "2            0         0  \n",
       "3            4         4  \n",
       "4            0         0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = data.drop(['tweet_id', 'writer','close_value', 'volume',\n",
    "       'open_value', 'high_value','low_value','change_label','class_sign','close_value_change'],axis=1).copy()\n",
    "\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd35787",
   "metadata": {},
   "source": [
    "<li> The dataframe has ticker_symbol column where the company name for which we need to predict is present for all rows while other columns are related to tweet information. </li>\n",
    "<li> Here we can see that only information related to tweets is present and does not contain any information related to prices. </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab6e1c2",
   "metadata": {},
   "source": [
    "### 2.1 Testing \"function1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd36d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2019-06-03'\n",
    "company = 'AAPL'\n",
    "\n",
    "X = temp[(temp.post_date == date) & (temp.ticker_symbol == company)].copy()\n",
    "y_true_change = data[(data.post_date == date) & (data.ticker_symbol == company)]['change_label'].unique()[0]\n",
    "y_true_sign = data[(data.post_date == date) & (data.ticker_symbol == company)]['class_sign'].unique()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65409718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ticker_symbol', 'post_date', 'body', 'comment_num', 'retweet_num',\n",
       "       'like_num'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcf37cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For company AAPL, the % change in closing price on 2019-06-03 was 1.0110241617638556 and predicted as 1.7048200368881226 \n",
      "\n",
      "For company AAPL, the sign for change in closing price on 2019-06-03 was 0 and predicted as 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calling function1\n",
    "\n",
    "pred = function1(X.copy())\n",
    "print(\"For company {}, the % change in closing price on {} was {} and predicted as {} \\n\".format(company,date,y_true_change,pred[1][0][0]))\n",
    "print(\"For company {}, the sign for change in closing price on {} was {} and predicted as {} \\n\".format(company,date,y_true_sign,pred[1][1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32cc5bd",
   "metadata": {},
   "source": [
    "<li> Here 0 sign change means negative change while 1 sign change means positive change. </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2838c96",
   "metadata": {},
   "source": [
    "### 2.1 Testing \"function2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fe972fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('final_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4fd42d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>ticker_symbol</th>\n",
       "      <th>writer</th>\n",
       "      <th>post_date</th>\n",
       "      <th>body</th>\n",
       "      <th>comment_num</th>\n",
       "      <th>retweet_num</th>\n",
       "      <th>like_num</th>\n",
       "      <th>close_value</th>\n",
       "      <th>volume</th>\n",
       "      <th>open_value</th>\n",
       "      <th>high_value</th>\n",
       "      <th>low_value</th>\n",
       "      <th>close_value_change</th>\n",
       "      <th>change_label</th>\n",
       "      <th>class_sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3861206</th>\n",
       "      <td>1134610603715309569</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>ArchieAndrews85</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>The patent portfolio of those two companies co...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>175.07</td>\n",
       "      <td>27043580</td>\n",
       "      <td>176.23</td>\n",
       "      <td>177.99</td>\n",
       "      <td>174.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3861207</th>\n",
       "      <td>1134610842643836929</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>PortfolioBuzz</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>Having 10 different news tabs open for $AAPL $...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>175.07</td>\n",
       "      <td>27043580</td>\n",
       "      <td>176.23</td>\n",
       "      <td>177.99</td>\n",
       "      <td>174.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3861208</th>\n",
       "      <td>1134612405697335296</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>ppolitics</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>$AAPL has $245 billion in cash on hand - I thi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>175.07</td>\n",
       "      <td>27043580</td>\n",
       "      <td>176.23</td>\n",
       "      <td>177.99</td>\n",
       "      <td>174.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3861209</th>\n",
       "      <td>1134612972020654083</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>SusanLiTV</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>Using #tariffs 2 battle illegal #immigration? ...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>175.07</td>\n",
       "      <td>27043580</td>\n",
       "      <td>176.23</td>\n",
       "      <td>177.99</td>\n",
       "      <td>174.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3861210</th>\n",
       "      <td>1134614110010826752</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>TalkMarkets</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>FANG Stocks Update: Leading The Marketwide Cha...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>175.07</td>\n",
       "      <td>27043580</td>\n",
       "      <td>176.23</td>\n",
       "      <td>177.99</td>\n",
       "      <td>174.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    tweet_id ticker_symbol           writer   post_date  \\\n",
       "3861206  1134610603715309569          AAPL  ArchieAndrews85  2019-06-01   \n",
       "3861207  1134610842643836929          AAPL    PortfolioBuzz  2019-06-01   \n",
       "3861208  1134612405697335296          AAPL        ppolitics  2019-06-01   \n",
       "3861209  1134612972020654083          AAPL        SusanLiTV  2019-06-01   \n",
       "3861210  1134614110010826752          AAPL      TalkMarkets  2019-06-01   \n",
       "\n",
       "                                                      body  comment_num  \\\n",
       "3861206  The patent portfolio of those two companies co...            1   \n",
       "3861207  Having 10 different news tabs open for $AAPL $...            0   \n",
       "3861208  $AAPL has $245 billion in cash on hand - I thi...            0   \n",
       "3861209  Using #tariffs 2 battle illegal #immigration? ...            6   \n",
       "3861210  FANG Stocks Update: Leading The Marketwide Cha...            0   \n",
       "\n",
       "         retweet_num  like_num  close_value    volume  open_value  high_value  \\\n",
       "3861206            0         2       175.07  27043580      176.23      177.99   \n",
       "3861207            0         0       175.07  27043580      176.23      177.99   \n",
       "3861208            0         1       175.07  27043580      176.23      177.99   \n",
       "3861209            4        38       175.07  27043580      176.23      177.99   \n",
       "3861210            0         2       175.07  27043580      176.23      177.99   \n",
       "\n",
       "         low_value  close_value_change  change_label  class_sign  \n",
       "3861206     174.99                 0.0           0.0           1  \n",
       "3861207     174.99                 0.0           0.0           1  \n",
       "3861208     174.99                 0.0           0.0           1  \n",
       "3861209     174.99                 0.0           0.0           1  \n",
       "3861210     174.99                 0.0           0.0           1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us take test set as last 6 months of available data\n",
    "\n",
    "data = data[data.post_date >= '2019-06-01']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8119e591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 214/214 [00:58<00:00,  3.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# extracting class labels in dict format from dataset in order to provide those as inputs\n",
    "# the dict format is y_true[date][company name] = true value\n",
    "# Hence it is nested dictionary\n",
    "\n",
    "# getting unique dates and companies\n",
    "dates = data['post_date'].unique()\n",
    "companies = data['ticker_symbol'].unique()\n",
    "\n",
    "# creating dictionaries to store class labels\n",
    "y_change_label = {}\n",
    "y_class_sign = {}\n",
    "for date in tqdm(dates):\n",
    "    # creating empty nested dictionaries\n",
    "    y_change_label[date] = {}\n",
    "    y_class_sign[date] = {}\n",
    "    for company in companies:\n",
    "        temp = data[(data.post_date == date) & (data.ticker_symbol == company)]\n",
    "\n",
    "        try:\n",
    "            y_change_label[date][company] = temp['change_label'].unique()[0]\n",
    "            y_class_sign[date][company] = temp['class_sign'].unique()[0]\n",
    "        except:\n",
    "            # for missing values of class labels, -1 was assigned and this is handled in function2\n",
    "            y_change_label[date][company] = -1\n",
    "            y_class_sign[date][company] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52508992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker_symbol</th>\n",
       "      <th>post_date</th>\n",
       "      <th>body</th>\n",
       "      <th>comment_num</th>\n",
       "      <th>retweet_num</th>\n",
       "      <th>like_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3861206</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>The patent portfolio of those two companies co...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3861207</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>Having 10 different news tabs open for $AAPL $...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3861208</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>$AAPL has $245 billion in cash on hand - I thi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3861209</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>Using #tariffs 2 battle illegal #immigration? ...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3861210</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>FANG Stocks Update: Leading The Marketwide Cha...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ticker_symbol   post_date  \\\n",
       "3861206          AAPL  2019-06-01   \n",
       "3861207          AAPL  2019-06-01   \n",
       "3861208          AAPL  2019-06-01   \n",
       "3861209          AAPL  2019-06-01   \n",
       "3861210          AAPL  2019-06-01   \n",
       "\n",
       "                                                      body  comment_num  \\\n",
       "3861206  The patent portfolio of those two companies co...            1   \n",
       "3861207  Having 10 different news tabs open for $AAPL $...            0   \n",
       "3861208  $AAPL has $245 billion in cash on hand - I thi...            0   \n",
       "3861209  Using #tariffs 2 battle illegal #immigration? ...            6   \n",
       "3861210  FANG Stocks Update: Leading The Marketwide Cha...            0   \n",
       "\n",
       "         retweet_num  like_num  \n",
       "3861206            0         2  \n",
       "3861207            0         0  \n",
       "3861208            0         1  \n",
       "3861209            4        38  \n",
       "3861210            0         2  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping non-required columns\n",
    "X = data.drop(['tweet_id', 'writer','close_value', 'volume',\n",
    "       'open_value', 'high_value','low_value','change_label','class_sign','close_value_change'],axis=1).copy()\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c8b84a",
   "metadata": {},
   "source": [
    "#### Input data format is similar to function1 the change here is class label as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa9e7835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 465826/465826 [01:27<00:00, 5347.40it/s]\n",
      "100%|| 214/214 [00:00<00:00, 295.18it/s]\n",
      "100%|| 214/214 [00:02<00:00, 101.28it/s]\n",
      "100%|| 214/214 [00:00<00:00, 585.28it/s]\n",
      "100%|| 214/214 [00:00<00:00, 606.46it/s]\n",
      "100%|| 214/214 [00:00<00:00, 589.86it/s]\n",
      "100%|| 214/214 [00:00<00:00, 608.47it/s]\n",
      "100%|| 214/214 [00:00<00:00, 591.75it/s]\n",
      "100%|| 214/214 [00:00<00:00, 595.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Deviation for % change in closing prices is :  0.8286616\n",
      "Number of correct signs predicted out of 1284 are : 873\n"
     ]
    }
   ],
   "source": [
    "# calling function2\n",
    "\n",
    "pred = function2(X,y_change_label,y_class_sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a15f0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted chnage mean deviation is 0.8286616206169128 and correct prediction ratio is 0.6799065420560748\n"
     ]
    }
   ],
   "source": [
    "print(\"predicted chnage mean deviation is {} and correct prediction ratio is {}\".format(pred[0],pred[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c322c8e5",
   "metadata": {},
   "source": [
    "#### These results states that tweeter does have an impact on stock markets but it is not the only factor affecting stock market. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e676f68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
